{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import textstat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Define file paths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata_folder_path = \"/gpfs/data/majorlab/biasaudit/data/raw_records/\"  \n",
    "edit_dist_ingredients = \"/gpfs/data/majorlab/biasaudit/data/2024-11-19_edit_distance_ingredients.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Define functions**\n",
    "Load all GPT generated summaries in the biasaudit>data folder into a dictionary. Takes folder name and number of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(folder_path, number_rows):\n",
    "    data_list = []\n",
    "    file_count = 0\n",
    "    \n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.json'):  # Check if the file is a JSON\n",
    "            file_path = os.path.join(folder_path, file_name)            \n",
    "            try:\n",
    "                # Open and load JSON content\n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = json.load(f)  # Load JSON as a dictionary\n",
    "                    if data:  # Check if data is not empty\n",
    "                        data_list.append(data)  # Add the dictionary to the list\n",
    "                    else:\n",
    "                        print(\"Warning: {} is empty or invalid.\".format(file_name))\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(\"Error decoding {}: {e}\".format(file_name))\n",
    "\n",
    "            # Increment the file counter\n",
    "            file_count += 1\n",
    "\n",
    "            # Stop if 100 files are processed (optional)\n",
    "            if file_count >= number_rows:\n",
    "                break\n",
    "                \n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(text):\n",
    "    # Replace all types of newlines and extra spaces with a single space\n",
    "    clean_text = re.sub(r'\\s+', ' ', text).strip()  \n",
    "\n",
    "    words = clean_text.split()\n",
    "\n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract relevant key / value pairs from the list of dictionaries with all the summaries. Fields correspond to the following:\n",
    "* csn = episode ID\n",
    "* qX = full text of question 1-4 (answers generated by GPT)\n",
    "* WC_QX = word count for questions 1-4\n",
    "* PF_note = GPT generated free text note summarising admission\n",
    "* FK_grade and FK_ease = Flesch Kincaid reading grade and reading ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_for_edit_dist(summary_dict):\n",
    "    postprocessed = summary_dict.get('postprocessed', {})\n",
    "\n",
    "    q1 = postprocessed.get('Q1', '')  \n",
    "    q2 = postprocessed.get('Q2', '')  \n",
    "    q3 = postprocessed.get('Q3', '')  \n",
    "    q4 = postprocessed.get('Q4', '')\n",
    "    combined_Q1_Q4 = \" \".join([\"What brought me to the hospital?\", \n",
    "        q1, 'Why was I hospitalized?', q2, 'What happened in the hospital?',\n",
    "        q3, 'What should I know after leaving the hospital?', q4])\n",
    "    processed_note_text = summary_dict.get('processed_note_text')\n",
    "\n",
    "    summary = {\n",
    "        'csn': summary_dict.get('csn'),\n",
    "        'q1': q1,\n",
    "        'q2': q2,\n",
    "        'q3': q3,\n",
    "        'q4': q4,\n",
    "        'PF_DC': combined_Q1_Q4, \n",
    "        'OG_DC': processed_note_text,\n",
    "        'wc_Q1': word_count(q1),\n",
    "        'wc_Q2': word_count(q2),\n",
    "        'wc_Q3': word_count(q3),\n",
    "        'wc_Q4': word_count(q4),\n",
    "        'wc_PF_DC': word_count(combined_Q1_Q4),\n",
    "        'wc_OG_DC': word_count(processed_note_text),\n",
    "        'fk_ease_PF_DC': textstat.flesch_reading_ease(combined_Q1_Q4),\n",
    "        'fk_grade_PF_DC': textstat.flesch_kincaid_grade(combined_Q1_Q4),\n",
    "        'fk_ease_OG_DC': textstat.flesch_reading_ease(processed_note_text),\n",
    "        'fk_grade_OG_DC': textstat.flesch_kincaid_grade(processed_note_text)\n",
    "    }\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a function to the list of summaries to generate a new list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_to_dict(dict_list, function_name):\n",
    "    new_summaries = []\n",
    "    for summary in dict_list:\n",
    "        new = function_name(summary)\n",
    "        new_summaries.append(new)\n",
    "    \n",
    "    return new_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group EMR extracts of each line of note into each encounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupby_csn_id(df):\n",
    "    df = df.sort_values(by=['pat_enc_csn_id', 'text_line'], ascending=True)\n",
    "\n",
    "    grouped_df = df.groupby('pat_enc_csn_id').agg(\n",
    "        q1_exists=('q1_exists', 'max'),  # Use max to retain True if any row has True\n",
    "        q2_exists=('q2_exists', 'max'),\n",
    "        q3_exists=('q3_exists', 'max'),\n",
    "        q4_exists=('q4_exists', 'max'),\n",
    "        note_text_concat=('note_text', ' '.join)  # Concatenate all note_text for the group\n",
    "    ).reset_index()\n",
    "    \n",
    "    return grouped_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the Q1-4 section of the DC summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_post_edits(text):\n",
    "    pattern = r\"A Simplified Guide(.*?)Visit Guide\\.\"\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    \n",
    "    for match in matches:\n",
    "        return(match.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trim superfluous text from EMR summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to fix this \n",
    "\n",
    "def trim_text(text):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return None  \n",
    "\n",
    "    phrases = ['to Your Hospital Stay',\n",
    "               ': Translating Your Hospital Stay',\n",
    "               'Make sure to read the full medication instructions printed below and in the NYU Langone Health After']\n",
    "    \n",
    "    for phrase in phrases:\n",
    "        text = text.replace(phrase, \"\")\n",
    "    \n",
    "    text = \" \".join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FK reading grade and reading ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_flesch_reading_ease(text):\n",
    "    try:\n",
    "        return textstat.flesch_reading_ease(text)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def safe_flesch_kincaid_grade(text):\n",
    "    try:\n",
    "        return textstat.flesch_kincaid_grade(text)\n",
    "    except:\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Levenshtein edit distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import Levenshtein as lev\n",
    "\n",
    "\n",
    "def calculate_lev(row, col1, col2):\n",
    "    val1 = row[col1]\n",
    "    val2 = row[col2]\n",
    "    \n",
    "    if not isinstance(val1, str):\n",
    "        val1 = \"\"  # Convert to empty string\n",
    "    if not isinstance(val2, str):\n",
    "        val2 = \"\"  # Convert to empty string\n",
    "    \n",
    "    # Calculate Levenshtein distance\n",
    "    return lev.distance(val1, val2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Running functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to generate table of GPT generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read json files into list of dictionaries\n",
    "summaries = read_json(rawdata_folder_path, 3687)\n",
    "\n",
    "# extract relevant key value pairs\n",
    "new_sum = apply_to_dict(summaries, shape_for_edit_dist)\n",
    "\n",
    "# make this into a dataframe, change the csn to a float not string\n",
    "summaries_df = pd.DataFrame(new_sum)\n",
    "summaries_df['csn'] = summaries_df['csn'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to extract relevant parts of summary from clinician edited version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload csv as dataframe\n",
    "edit_dist_ingredients = pd.read_csv(edit_dist_ingredients)\n",
    "\n",
    "dist_csn = groupby_csn_id(edit_dist_ingredients)\n",
    "\n",
    "dist_csn['note_for_edit_dist'] = dist_csn['note_text_concat'].apply(extract_post_edits)\n",
    "dist_csn['csn'] = dist_csn['pat_enc_csn_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge dataframes together, drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(dist_csn, summaries_df, on='csn', how='left')\n",
    "merged = merged.drop(['pat_enc_csn_id', 'q1_exists', 'q2_exists', 'q3_exists', 'q4_exists'], axis=1)\n",
    "merged['trimmed_note'] = merged['note_for_edit_dist'].apply(trim_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate FK reading grade and reading ease scores, Levenshtein edit distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['trimmed_note_fk_ease'] = merged['trimmed_note'].apply(safe_flesch_reading_ease)\n",
    "merged['trimmed_note_fk_grade'] = merged['trimmed_note'].apply(safe_flesch_kincaid_grade)\n",
    "merged['Levenshtein_edit'] = merged.apply(calculate_lev, args=('combined_Q1_Q4', 'trimmed_note'), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting discharge instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_df.to_csv('20250114_summaries.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
